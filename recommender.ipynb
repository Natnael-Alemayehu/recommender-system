{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":81285,"databundleVersionId":8778365,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install packages here\n# Packages for data processing\nimport numpy as np\nimport pandas as pd\nimport datetime\nfrom sklearn import preprocessing\nfrom sklearn.datasets import make_blobs\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\nfrom scipy.sparse import csr_matrix\nimport scipy as sp\n\n\n# Packages for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Packages for modeling\nfrom surprise import Reader\nfrom surprise import Dataset\nfrom surprise import KNNWithMeans\nfrom surprise import KNNBasic\nfrom surprise.model_selection import cross_validate\nfrom surprise.model_selection import GridSearchCV\nfrom surprise import SVD\nfrom surprise import SVDpp\nfrom surprise import NMF\nfrom surprise import SlopeOne\nfrom surprise import CoClustering\nimport heapq\n\n# Packages for model evaluation\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom time import time\n\n# Package to suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Packages for saving models\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2024-06-22T09:17:28.768285Z","iopub.execute_input":"2024-06-22T09:17:28.768825Z","iopub.status.idle":"2024-06-22T09:17:32.408603Z","shell.execute_reply.started":"2024-06-22T09:17:28.768783Z","shell.execute_reply":"2024-06-22T09:17:32.406701Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/alx-movie-recommendation-project-2024/train.csv')\n# movies_df = pd.read_csv('/kaggle/input/alx-movie-recommendation-project-2024/movies.csv')\n# imdb_df = pd.read_csv('/kaggle/input/alx-movie-recommendation-project-2024/imdb_data.csv')\ntest_df = pd.read_csv('/kaggle/input/alx-movie-recommendation-project-2024/test.csv')\n# links_df = pd.read_csv('/kaggle/input/alx-movie-recommendation-project-2024/links.csv')\n# tags = pd.read_csv('/kaggle/input/alx-movie-recommendation-project-2024/tags.csv')\n# genome_scores = pd.read_csv('/kaggle/input/alx-movie-recommendation-project-2024/genome_scores.csv')\n# genome_tags = pd.read_csv('/kaggle/input/alx-movie-recommendation-project-2024/genome_tags.csv')\n# sample_submissions = pd.read_csv('/kaggle/input/alx-movie-recommendation-project-2024/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-06-22T09:17:32.410989Z","iopub.execute_input":"2024-06-22T09:17:32.411689Z","iopub.status.idle":"2024-06-22T09:17:42.764486Z","shell.execute_reply.started":"2024-06-22T09:17:32.411641Z","shell.execute_reply":"2024-06-22T09:17:42.763185Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"imdb_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA\n\n\n### Outliers\n- **Identify outliers**: Outliers are data points that differ significantly from other observations. They can skew and mislead the training process of a machine learning model.\n- **Detecting outliers**: Use statistical methods such as Z-scores or IQR (Interquartile Range) to detect outliers.\n- **Handling outliers**: Decide whether to remove or transform the outliers depending on their impact on the dataset.\n\n### Understanding Relationships Between Various Attributes and Structure of the Data\n- **Correlation Analysis**: Use correlation matrices to understand the relationships between numerical attributes.\n- **Visualization Techniques**: Employ scatter plots, pair plots, and heatmaps to visualize and explore relationships.\n- **Data Structure**: Understand the structure of the data, including the distribution of values and the presence of any missing values.\n\n### Recognizing Important Variables\n- **Feature Importance**: Use techniques like Random Forests, Gradient Boosting, or SHAP values to determine feature importance.\n- **Domain Knowledge**: Incorporate domain expertise to identify which variables are likely to be important.\n- **Statistical Tests**: Conduct statistical tests to identify variables that have significant effects on the target variable.\n\nBy understanding the data through these steps, we ensure a robust foundation for building and evaluating machine learning models.\n","metadata":{"execution":{"iopub.status.busy":"2024-06-19T07:21:47.283273Z","iopub.execute_input":"2024-06-19T07:21:47.283677Z","iopub.status.idle":"2024-06-19T07:21:47.293413Z","shell.execute_reply.started":"2024-06-19T07:21:47.283645Z","shell.execute_reply":"2024-06-19T07:21:47.292097Z"}}},{"cell_type":"code","source":"print(\"Train: \")\nprint(str(train_df.isnull().sum()))\nprint(\"************\")\nprint(\"Test: \")\nprint(str(test_df.isnull().sum()))\nprint(\"************\")\nprint(\"Movies: \")\nprint(str(movies_df.isnull().sum()))\nprint(\"************\")\nprint(\"Links: \")\nprint(str(links_df.isnull().sum()))\nprint(\"************\")\nprint(\"IMDB: \")\nprint(str(imdb_df.isnull().sum()))\nprint(\"************\")\nprint(\"Genome scores: \")\nprint(str(genome_scores.isnull().sum()))\nprint(\"************\")\nprint(\"Genome tags: \")\nprint(str(genome_tags.isnull().sum()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data preparation is the process of preparing raw data so that it is suitable for further processing and analysis. Key steps include:\n\n- **Collecting**: Gathering raw data from various sources.\n- **Cleaning**: Removing or correcting any errors or inconsistencies in the data. This includes handling missing values, correcting data types, and removing duplicates.\n- **Labeling**: Annotating data with labels that are required for supervised machine learning tasks. This involves identifying and marking the target variable.\n- **Transforming**: Converting raw data into a format that is suitable for analysis. This includes normalization, scaling, encoding categorical variables, and feature engineering.\n- **Exploring**: Analyzing the data to understand its structure and relationships. This step includes generating descriptive statistics and visualizing the data to identify patterns and insights.\n- **Visualizing**: Creating graphical representations of the data to better understand distributions, trends, and relationships among variables. Common techniques include histograms, bar charts, scatter plots, and heatmaps.\n\nBy following these steps, raw data is transformed into a structured format that is ready for machine learning algorithms and further analysis.\n","metadata":{}},{"cell_type":"code","source":"movies_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create dataframe containing only the movieId and genres\nmovies_genres = pd.DataFrame(movies_df[['movieId', 'genres']],\n                             columns=['movieId', 'genres'])\n\n# Split genres seperated by \"|\" and create a list containing the genres allocated to each movie\nmovies_genres.genres = movies_genres.genres.apply(lambda x: x.split('|'))\n\n# Create expanded dataframe where each movie-genre combination is in a seperate row\nmovies_genres = pd.DataFrame([(tup.movieId, d) for tup in movies_genres.itertuples() for d in tup.genres],\n                             columns=['movieId', 'genres'])\n\nmovies_genres.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lets plot genres from most common to least common**","metadata":{}},{"cell_type":"code","source":"plot = plt.figure(figsize=(15, 10))\nplt.title('Most common genres\\n', fontsize=20)\nsns.countplot(y=\"genres\", data=movies_genres,\n              order=movies_genres['genres'].value_counts(ascending=False).index,\n              palette='Reds_r')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelling phase\n You only need to apply one version\nbe it Content based or Collabrative method\n\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom surprise import Reader, Dataset, SVD, accuracy\nfrom surprise.model_selection import train_test_split\n\n# Sample books data\nbooks_data = {\n    'bookId': [1, 2, 3, 4, 5, 6, 7],\n    'title': ['To Kill a Mockingbird', '1984', 'The Great Gatsby', 'Pride and Prejudice', 'The Catcher in the Rye', 'Animal farm'\n              ,'Harry Potter']\n}\n\n# Create the books DataFrame\nbooks = pd.DataFrame(books_data)\n\n# Sample user profiles\nuser_profiles = {\n    1: {'bookId': [2, 3, 4], 'rating': [5, 4, 3]},\n    2: {'bookId': [1, 3, 5], 'rating': [4, 5, 3]}\n}\n\n# Combine user profiles into a single DataFrame\nall_ratings = []\n\nfor user_id, profile in user_profiles.items():\n    for book_id, rating in zip(profile['bookId'], profile['rating']):\n        all_ratings.append({'userId': user_id, 'bookId': book_id, 'rating': rating})\n\nratings_df = pd.DataFrame(all_ratings)\n\n# Define the Reader and Dataset\nreader = Reader(rating_scale=(0.5, 5.0))\ndata = Dataset.load_from_df(ratings_df[['userId', 'bookId', 'rating']], reader)\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into training and test sets\ntrainset, testset = train_test_split(data, test_size=0.2, )\n\n# Train the SVD model\nsvd = SVD()\nsvd.fit(trainset)\n\n# Make predictions on the test set\npredictions = svd.test(testset)\n\n# Compute and print the RMSE\nrmse = accuracy.rmse(predictions)\nprint(f\"RMSE: {rmse}\")\n\npred = pd.DataFrame(predictions)\npred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Recommendation Function","metadata":{}},{"cell_type":"code","source":"# Function to get collaborative recommendations for a user profile\ndef get_collaborative_recommendations(user_id, svd, books, ratings_df, n=10):\n    recommendations = []\n    book_ids = books['bookId'].unique()\n\n    for book_id in book_ids:\n        prediction = svd.predict(user_id, book_id)\n        actual_rating = ratings_df[(ratings_df['userId'] == user_id) & (ratings_df['bookId'] == book_id)]['rating']\n        actual_rating = actual_rating.values[0] if not actual_rating.empty else None\n        recommendations.append((books[books['bookId'] == book_id]['title'].values[0], prediction.est, actual_rating))\n\n    recommendations = sorted(recommendations, key=lambda x: x[1], reverse=True)\n\n    return recommendations[:n]\n\n# Generate recommendations for each user profile and collect them in a list\nall_recommendations = []\n\nfor user_id in user_profiles.keys():\n    recommendations = get_collaborative_recommendations(user_id, svd, books, ratings_df)\n    for title, predicted_rating, actual_rating in recommendations:\n        all_recommendations.append({\n            'userId': user_id,\n            'Recommended Book': title,\n            'Predicted_Rating': predicted_rating,\n            'Actual Rating': actual_rating\n        })\n\n# Convert the list of recommendations to a DataFrame\nrecommendations_df = pd.DataFrame(all_recommendations)\n\nrecommendations_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate your outputs here","metadata":{}},{"cell_type":"markdown","source":"Prepare Submission File\nWe make submissions in CSV files. Your submissions usually have two columns: an ID column and a prediction column. The ID field comes from the test data (keeping whatever name the ID field had in that data, which for the data is the string 'Id'). The prediction column will use the name of the target field.\n\nWe will create a DataFrame with this data, and then use the dataframe's to_csv method to write our submission file. Explicitly include the argument index=False to prevent pandas from adding another column in our csv file.","metadata":{}},{"cell_type":"code","source":"# This is an example\nmy_submission = pd.DataFrame({'id': recommendations_df.userId,'predict': recommendations_df.Predicted_Rating})\n#you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tips\n- NB:Read the description well\n    - REMEMBER: Concatenated ID\n    - Evatualtion metrics\n- Sampling is your friend -> start small and scale up\n- Data ingestioon, pleasse ensure the correct path is dependant on the environment\n- Ensure test output matches dimension of test set for Kaggle submission\n- 20 Submissions per day\n- This is individual project\n- Ensure email correlates to Athena for effective tracking\n- If you use a shuffler ensure test output aligns with test sample ordering\n  - (from sklearn.utils import shuffle), for randomness\n- Make sure your notebook is in the same folder\n- Analyse your data well\n- Make sure you have gone through your content\n","metadata":{}},{"cell_type":"markdown","source":"# Testing Ideas","metadata":{}},{"cell_type":"code","source":"sampled_df = train_df.sample(frac=0.5, random_state=42)\nsampled_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from surprise import Reader, Dataset, SVD, accuracy\nfrom surprise.model_selection import train_test_split\n\n# Define the Reader object\nreader = Reader(rating_scale=(0.5, 5.0))\n\n# Load the dataset\ndata = Dataset.load_from_df(train_df[['userId', 'movieId', 'rating']], reader)\n\n# Split the data into train and test sets\ntrainset, testset = train_test_split(data, test_size=0.2)\n\n# Initialize the SVD algorithm\nalgo = SVD()\n\n# Train the algorithm on the training set\nalgo.fit(trainset)\n\n# Predict ratings for the test set\npredictions = algo.test(testset)\n\n# Calculate RMSE\nrmse = accuracy.rmse(predictions)\nprint(f'RMSE: {rmse}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nUsing sample fraction of 0.05\nRMSE: 0.9290\nRMSE: 0.9290476877052269\n\nUsing sample fraction of 0.25\nRMSE: 0.8990\nRMSE: 0.8989952813217597\n\nUsing sample fraction of 0.05\nRMSE: 0.8760\nRMSE: 0.8759642544462234\n\nUsing the whole dataset from the train_df\nRMSE: 0.8347\nRMSE: 0.8346884683865706\n\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing the whole dataset from the method above","metadata":{"execution":{"iopub.status.busy":"2024-06-19T08:40:38.209438Z","iopub.execute_input":"2024-06-19T08:40:38.210148Z","iopub.status.idle":"2024-06-19T08:40:38.214750Z","shell.execute_reply.started":"2024-06-19T08:40:38.210108Z","shell.execute_reply":"2024-06-19T08:40:38.213612Z"}}},{"cell_type":"code","source":"from surprise import Reader, Dataset, SVD, accuracy\nfrom surprise.model_selection import train_test_split\nimport timeit\n\n# This is to start the execution time for the cell\nstart_time = timeit.default_timer()\n\n\n# Step 2: Prepare Data\n# Define the Reader object\nreader = Reader(rating_scale=(0.5, 5.0))\n\n# Load the dataset\ndata = Dataset.load_from_df(train_df[['userId', 'movieId', 'rating']], reader)\n\n# Step 3: Train-Test Split (for model evaluation, not for actual prediction)\ntrainset, _ = train_test_split(data, test_size=0.2)\n\n# Initialize the SVD algorithm\nalgo = SVD()\n\n# Train the algorithm on the trainset\nalgo.fit(trainset)\n\n# Step 4: Predict Ratings for the Test Set\n# Prepare a list of (userId, movieId) pairs from test_df\ntestset = list(zip(test_df['userId'], test_df['movieId'], [0]*len(test_df)))  # the third element is a dummy rating\n\n# Predict ratings for the testset\npredictions = algo.test(testset)\n\n# Extract predictions\npredicted_ratings = [pred.est for pred in predictions]\n\n# Add predicted ratings to test_df\ntest_df['rating'] = predicted_ratings\n\n# Optional: Save the predictions to a new CSV file\ntest_df.to_csv('predicted_ratings.csv', index=False)\n\nprint(test_df.head())\n\nelapsed = timeit.default_timer() - start_time\nprint(elapsed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testdf = test_df.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combine_columns(row):\n    return f\"{int(row['userId'])}_{int(row['movieId'])}\"\n\ntestdf['combined'] = testdf.apply(combine_columns, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testdf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select only the 'combined' and 'rating' columns\nresult_df = testdf[['combined', 'rating']]\nresult_df = result_df.rename(columns={'combined': 'Id'})\n# Print the resulting DataFrame\n\nresult_df.to_csv('predicted_ratings.csv', index=False)\n\nprint(result_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The above code has given me an RMSE of 0.083 -> Now I want to improve my result by using a hyper parameter tuning","metadata":{"execution":{"iopub.status.busy":"2024-06-19T09:35:55.460481Z","iopub.execute_input":"2024-06-19T09:35:55.460913Z","iopub.status.idle":"2024-06-19T09:35:55.466263Z","shell.execute_reply.started":"2024-06-19T09:35:55.460878Z","shell.execute_reply":"2024-06-19T09:35:55.464827Z"}}},{"cell_type":"code","source":"# Timer start\nstart_time = timeit.default_timer()\n# Data is already loaded\n\n# Prepare Data\nreader = Reader(rating_scale=(0.5, 5.0))\ndata = Dataset.load_from_df(train_df[['userId', 'movieId', 'rating']], reader)\n\n# Train-Test Split\ntrainset, testset = train_test_split(data, test_size=0.2)\n\n# Hyperparameter Tuning\nparam_grid = {\n    'n_factors': [50, 100, 150],\n    'n_epochs': [20, 30, 40],\n    'lr_all': [0.002, 0.005, 0.01],\n    'reg_all': [0.02, 0.05, 0.1]\n}\ngs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\ngs.fit(data)\nbest_params = gs.best_params['rmse']\n\n# Train the best model\nalgo = SVD(**best_params)\nalgo.fit(trainset)\n\n# Predict Ratings for the Test Set\ntestset = list(zip(test_df['userId'], test_df['movieId'], [0]*len(test_df)))\npredictions = algo.test(testset)\npredicted_ratings = [pred.est for pred in predictions]\n\n# Add predicted ratings to test_df\ntest_df['rating'] = predicted_ratings\ntest_df['combined'] = test_df.apply(lambda row: f\"{int(row['userId'])}_{int(row['movieId'])}\", axis=1)\nresult_df = test_df[['combined', 'rating']]\n\n# Save the results to a CSV file\nresult_df.to_csv('predicted_ratings_2.csv', index=False)\nprint(result_df.shape)\nprint(result_df.head())\n\n# Timing the amount of time it takes to complete the process. The one without the grid was 304sec.\nelapsed = timeit.default_timer() - start_time\nprint(elapsed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The above code took a very long time so I tried to decrease the time complexity using the below code","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom surprise import Reader, Dataset, SVD, accuracy\nfrom surprise.model_selection import train_test_split, GridSearchCV\nimport timeit\n\n# Timer start\nstart_time = timeit.default_timer()\n\n# Prepare Data\nreader = Reader(rating_scale=(0.5, 5.0))\ndata = Dataset.load_from_df(train_df[['userId', 'movieId', 'rating']], reader)\n\n# Sample 20% of the data for hyperparameter tuning\nsample_data = train_df.sample(frac=0.3, random_state=42)\nsample_data = Dataset.load_from_df(sample_data[['userId', 'movieId', 'rating']], reader)\nsample_trainset = sample_data.build_full_trainset()\n\n# Hyperparameter Tuning\nparam_grid = {\n    'n_factors': [50, 100],\n    'n_epochs': [20, 30],\n    'lr_all': [0.002, 0.005],\n    'reg_all': [0.02, 0.05]\n}\ngs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=2, n_jobs=-1)\ngs.fit(sample_data)\nbest_params = gs.best_params['rmse']\n\n# Train-Test Split on Full Data\ntrainset, testset = train_test_split(data, test_size=0.2)\n\n# Train the best model\nalgo = SVD(**best_params)\nalgo.fit(trainset)\n\n# Predict Ratings for the Test Set\ntestset = list(zip(test_df['userId'], test_df['movieId'], [0]*len(test_df)))\npredictions = algo.test(testset)\npredicted_ratings = [pred.est for pred in predictions]\n\n# Add predicted ratings to test_df\ntest_df['rating'] = predicted_ratings\ntest_df['combined'] = test_df.apply(lambda row: f\"{int(row['userId'])}_{int(row['movieId'])}\", axis=1)\nresult_df = test_df[['combined', 'rating']]\n\nresult_df = result_df.rename(columns={'combined': 'Id'})\n\n# Save the results to a CSV file\nresult_df.to_csv('predicted_ratings_2.csv', index=False)\nprint(result_df.shape)\nprint(result_df.head())\n\n# Timing the amount of time it takes to complete the process\nelapsed = timeit.default_timer() - start_time\nprint(elapsed)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_df.to_csv('predicted_ratings_2.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom surprise import Reader, Dataset, SVD, SVDpp, NMF, accuracy\nfrom surprise.model_selection import train_test_split, GridSearchCV, cross_validate\nimport timeit\n\n# Timer start\nstart_time = timeit.default_timer()\n\n# Prepare Data\nreader = Reader(rating_scale=(0.5, 5.0))\ndata = Dataset.load_from_df(train_df[['userId', 'movieId', 'rating']], reader)\n\n# Sample 30% of the data for hyperparameter tuning\nsample_data = train_df.sample(frac=1, random_state=42)\nsample_data = Dataset.load_from_df(sample_data[['userId', 'movieId', 'rating']], reader)\nsample_trainset = sample_data.build_full_trainset()\n\n# Expanded Hyperparameter Tuning\nparam_grid = {\n    'n_factors': [20, 50, 100, 150],\n    'n_epochs': [10, 20, 30, 40],\n    'lr_all': [0.001, 0.002, 0.005, 0.01],\n    'reg_all': [0.01, 0.02, 0.05, 0.1]\n}\n\ngs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=2, n_jobs=-1)\ngs.fit(sample_data)\nbest_params = gs.best_params['rmse']\n\n# Train-Test Split on Full Data\ntrainset, testset = train_test_split(data, test_size=0.2)\n\n# Train the best model\nalgo = SVD(**best_params)\nalgo.fit(trainset)\n\n# Predict Ratings for the Test Set\ntestset = list(zip(test_df['userId'], test_df['movieId'], [0]*len(test_df)))\npredictions = algo.test(testset)\npredicted_ratings = [pred.est for pred in predictions]\n\n# Add predicted ratings to test_df\ntest_df['rating'] = predicted_ratings\ntest_df['combined'] = test_df.apply(lambda row: f\"{int(row['userId'])}_{int(row['movieId'])}\", axis=1)\nresult_df = test_df[['combined', 'rating']]\nresult_df = result_df.rename(columns={'combined': 'Id'})\n\n# Save the results to a CSV file\nresult_df.to_csv('predicted_ratings_4.csv', index=False)\nprint(result_df.shape)\nprint(result_df.head())\n\n# Timing the amount of time it takes to complete the process\nelapsed = timeit.default_timer() - start_time\nprint(f\"{elapsed} in seconds in Mins = {elapsed / 60} mins\")\n\n# Cross-validate SVD\nalgo = SVD()\ncv_results = cross_validate(algo, data, measures=['RMSE'], cv=5, verbose=True)\nprint(f\"Cross-validated RMSE: {cv_results['test_rmse'].mean()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom surprise import Reader, Dataset, SVD, SVDpp, NMF, KNNBasic, accuracy\nfrom surprise.model_selection import train_test_split, cross_validate\nimport timeit\n\n# Timer start\nstart_time = timeit.default_timer()\n\n# Prepare Data\nreader = Reader(rating_scale=(0.5, 5.0))\ndata = Dataset.load_from_df(train_df[['userId', 'movieId', 'rating']], reader)\n\n# Train-Test Split on Full Data\ntrainset, testset = train_test_split(data, test_size=0.2)\n\n# Define and train SVD model\nsvd = SVD(n_factors=100, n_epochs=20, lr_all=0.005, reg_all=0.02)\nsvd.fit(trainset)\n\n# Define and train SVD++ model\nsvdpp = SVDpp(n_factors=100, n_epochs=20, lr_all=0.005, reg_all=0.02)\nsvdpp.fit(trainset)\n\n# Define and train NMF model\nnmf = NMF(n_factors=100, n_epochs=20, reg_pu=0.02, reg_qi=0.02)\nnmf.fit(trainset)\n\n# Define and train KNNBasic model\nsim_options = {'name': 'cosine', 'user_based': False}\nknn = KNNBasic(sim_options=sim_options)\nknn.fit(trainset)\n\n# Predict Ratings for the Test Set\ntestset = list(zip(test_df['userId'], test_df['movieId'], [0]*len(test_df)))\n\npredictions_svd = svd.test(testset)\npredictions_svdpp = svdpp.test(testset)\npredictions_nmf = nmf.test(testset)\npredictions_knn = knn.test(testset)\n\n# Blend the predictions (simple average)\nblended_predictions = []\nfor svd_pred, svdpp_pred, nmf_pred, knn_pred in zip(predictions_svd, predictions_svdpp, predictions_nmf, predictions_knn):\n    blended_est = (svd_pred.est + svdpp_pred.est + nmf_pred.est + knn_pred.est) / 4\n    blended_predictions.append((svd_pred.uid, svd_pred.iid, blended_est))\n\n# Calculate RMSE for blended model\ntest_df['rating'] = [pred[2] for pred in blended_predictions]\npredicted_ratings = [pred[2] for pred in blended_predictions]\n\n# Add predicted ratings to test_df\ntest_df['rating'] = predicted_ratings\ntest_df['combined'] = test_df.apply(lambda row: f\"{int(row['userId'])}_{int(row['movieId'])}\", axis=1)\nresult_df = test_df[['combined', 'rating']]\nresult_df = result_df.rename(columns={'combined': 'Id'})\n\n# Save the results to a CSV file\nresult_df.to_csv('predicted_ratings_5.csv', index=False)\nprint(result_df.shape)\nprint(result_df.head())\n\n# Timing the amount of time it takes to complete the process\nelapsed = timeit.default_timer() - start_time\nprint(f\"Elapsed time: {elapsed} seconds\")\n\n# Evaluate the RMSE of the blended model\nblended_rmse = accuracy.rmse([pred[2] for pred in blended_predictions])\nprint(f\"Blended Model RMSE: {blended_rmse}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing SVD++","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom surprise import Reader, Dataset, SVDpp, accuracy\nfrom surprise.model_selection import train_test_split\nimport timeit\n\n# Timer start\nstart_time = timeit.default_timer()\n\n# Prepare Data\nreader = Reader(rating_scale=(0.5, 5.0))\ndata = Dataset.load_from_df(train_df[['userId', 'movieId', 'rating']], reader)\n\n# Sample 30% of the data for training\nsample_data = train_df.sample(frac=0.2, random_state=42)\nsample_data = Dataset.load_from_df(sample_data[['userId', 'movieId', 'rating']], reader)\ntrainset = sample_data.build_full_trainset()\n\n# Train the SVD++ model with predefined parameters on the sampled data\nalgo = SVDpp()\nalgo.fit(trainset)\n\n# Prepare test set\ntestset = list(zip(test_df['userId'], test_df['movieId'], [0]*len(test_df)))\npredictions = algo.test(testset)\npredicted_ratings = [pred.est for pred in predictions]\n\n# Add predicted ratings to test_df\ntest_df['rating'] = predicted_ratings\ntest_df['ID'] = test_df.apply(lambda row: f\"{int(row['userId'])}_{int(row['movieId'])}\", axis=1)\nresult_df = test_df[['ID', 'rating']]\n\n# Save the results to a CSV file\nresult_df.to_csv('SVDpp.csv', index=False)\nprint(result_df.shape)\nprint(result_df.head())\n\n# Timing the amount of time it takes to complete the process\nelapsed = timeit.default_timer() - start_time\nprint(f\"{elapsed} seconds or {elapsed / 60} minutes\")\n\n# Calculate and print RMSE\ntest_rmse = accuracy.rmse(predictions)\nprint(f\"SVD++ Model RMSE: {test_rmse}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-22T08:45:28.183343Z","iopub.execute_input":"2024-06-22T08:45:28.186089Z","iopub.status.idle":"2024-06-22T09:03:02.951533Z","shell.execute_reply.started":"2024-06-22T08:45:28.186009Z","shell.execute_reply":"2024-06-22T09:03:02.949229Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"(5000019, 2)\n       ID    rating\n0  1_2011  3.738186\n1  1_4144  4.059203\n2  1_5767  3.275729\n3  1_6711  3.561474\n4  1_7318  3.204849\n1050.5379753010002 seconds or 17.50896625501667 minutes\nRMSE: 3.5929\nSVD++ Model RMSE: 3.5929230983854774\n","output_type":"stream"}]},{"cell_type":"code","source":"# This was build with fraction of 0.1 with custom parameters\n#        ID    rating\n# 0  1_2011  3.760092\n# 1  1_4144  4.003556\n# 2  1_5767  3.545112\n# 3  1_6711  4.325373\n# 4  1_7318  3.069776\n# 900.9963529969991 seconds or 15.016605883283319 minutes\n# RMSE: 3.5714\n# SVD++ Model RMSE: 3.571350377637289","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is with SVD no-params\n#        ID    rating\n# 0  1_2011  3.738186\n# 1  1_4144  4.059203\n# 2  1_5767  3.275729\n# 3  1_6711  3.561474\n# 4  1_7318  3.204849\n# 1050.5379753010002 seconds or 17.50896625501667 minutes\n# RMSE: 3.5929\n# SVD++ Model RMSE: 3.5929230983854774","metadata":{"execution":{"iopub.status.busy":"2024-06-22T09:04:39.829816Z","iopub.execute_input":"2024-06-22T09:04:39.831227Z","iopub.status.idle":"2024-06-22T09:04:39.841906Z","shell.execute_reply.started":"2024-06-22T09:04:39.831163Z","shell.execute_reply":"2024-06-22T09:04:39.839214Z"},"trusted":true},"execution_count":7,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[7], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    This is with SVD no-params\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (940679574.py, line 1)","output_type":"error"}]},{"cell_type":"code","source":"import pandas as pd\nfrom surprise import Reader, Dataset, NMF, accuracy\nfrom surprise.model_selection import train_test_split\nimport timeit\n\n# Timer start\nstart_time = timeit.default_timer()\n\n# Prepare Data\nreader = Reader(rating_scale=(0.5, 5.0))\ndata = Dataset.load_from_df(train_df[['userId', 'movieId', 'rating']], reader)\n\n# Sample 30% of the data for training\n# sample_data = train_df.sample(frac=1, random_state=42)\n# sample_data = Dataset.load_from_df(data[['userId', 'movieId', 'rating']], reader)\ntrainset = data.build_full_trainset()\n\n# Define the NMF algorithm\nalgo = SlopeOne()\n\n# Train the NMF model with predefined parameters on the sampled data\nalgo.fit(trainset)\n\n# Prepare test set\ntestset = list(zip(test_df['userId'], test_df['movieId'], [0]*len(test_df)))\npredictions = algo.test(testset)\npredicted_ratings = [pred.est for pred in predictions]\n\n# Add predicted ratings to test_df\ntest_df['rating'] = predicted_ratings\ntest_df['ID'] = test_df.apply(lambda row: f\"{int(row['userId'])}_{int(row['movieId'])}\", axis=1)\nresult_df = test_df[['ID', 'rating']]\n\n# Save the results to a CSV file\nresult_df.to_csv('SlopeOne.csv', index=False)\nprint(result_df.shape)\nprint(result_df.head())\n\n# Timing the amount of time it takes to complete the process\nelapsed = timeit.default_timer() - start_time\nprint(f\"{elapsed} seconds or {elapsed / 60} minutes\")\n\n# Calculate and print RMSE\ntest_rmse = accuracy.rmse(predictions)\nprint(f\"NMF Model RMSE: {test_rmse}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-22T09:53:02.842081Z","iopub.execute_input":"2024-06-22T09:53:02.842682Z"},"trusted":true},"execution_count":null,"outputs":[]}]}